{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920863fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chapters to: final/4nMwZhF_D-g_chapters.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# === Config ===\n",
    "INPUT_FILE = \"transcript.txt\"\n",
    "FORMATTED_FILE = \"formatted_transcript.txt\"\n",
    "FINAL_JSON = \"final/4nMwZhF_D-g_chapters.json\"\n",
    "CHUNK_SIZE = 300  # seconds\n",
    "\n",
    "# === Load mBART (for summarization + translation) ===\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# === Load KeyBERT (for title generation) ===\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# === Utility Functions ===\n",
    "def parse_timestamp(ts):\n",
    "    h, m, s = map(int, ts.split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "def seconds_to_timestamp(seconds):\n",
    "    return str(timedelta(seconds=seconds))\n",
    "\n",
    "def summarize_and_translate(text):\n",
    "    tokenizer.src_lang = \"hi_IN\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
    "    summary_hi = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    tokenizer.src_lang = \"hi_IN\"\n",
    "    translated = model.generate(tokenizer(summary_hi, return_tensors=\"pt\").input_ids, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "def get_title(text):\n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=1)\n",
    "    return keywords[0][0] if keywords else \"Untitled\"\n",
    "\n",
    "# === Step 1: Format the Transcript ===\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Save formatted file (optional step if already clean)\n",
    "with open(FORMATTED_FILE, \"w\", encoding=\"utf-8\") as out:\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            out.write(line + '\\n')\n",
    "\n",
    "# === Step 2: Chunk Transcript into N-second Blocks ===\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_duration = 0\n",
    "chunk_start = None\n",
    "\n",
    "for line in lines:\n",
    "    match = re.match(r\"\\[(\\d+:\\d+:\\d+) - (\\d+:\\d+:\\d+)\\]: (.+)\", line)\n",
    "    if not match:\n",
    "        continue\n",
    "\n",
    "    start, end, content = match.groups()\n",
    "    start_sec = parse_timestamp(start)\n",
    "    end_sec = parse_timestamp(end)\n",
    "    duration = end_sec - start_sec\n",
    "\n",
    "    if chunk_start is None:\n",
    "        chunk_start = start_sec\n",
    "\n",
    "    if current_duration + duration > CHUNK_SIZE:\n",
    "        chunks.append((chunk_start, current_chunk))\n",
    "        current_chunk = [(start_sec, content)]\n",
    "        current_duration = duration\n",
    "        chunk_start = start_sec\n",
    "    else:\n",
    "        current_chunk.append((start_sec, content))\n",
    "        current_duration += duration\n",
    "\n",
    "if current_chunk:\n",
    "    chunks.append((chunk_start, current_chunk))\n",
    "\n",
    "# === Step 3: Summarize, Translate, Title, and Save ===\n",
    "Path(\"final\").mkdir(exist_ok=True)\n",
    "\n",
    "results = []\n",
    "for chunk_start, segment in chunks:\n",
    "    full_text = \" \".join([text for _, text in segment])\n",
    "    translated_summary = summarize_and_translate(full_text)\n",
    "    title = get_title(translated_summary)\n",
    "\n",
    "    results.append({\n",
    "        \"start_time\": seconds_to_timestamp(chunk_start),\n",
    "        \"title\": title,\n",
    "        \"summary\": translated_summary\n",
    "    })\n",
    "\n",
    "with open(FINAL_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… Saved chapters to: {FINAL_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e56245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ Final chapter file saved: 4nMwZhF_D-g_chapters.json\n"
     ]
    }
   ],
   "source": [
    "def format_timestamp_properly(ts):\n",
    "    \"\"\"Ensure timestamp is in hh:mm:ss with leading zeroes.\"\"\"\n",
    "    parts = ts.split(\":\")\n",
    "    return f\"{int(parts[0]):02d}:{int(parts[1]):02d}:{int(parts[2]):02d}\"\n",
    "\n",
    "# Format and save to required JSON structure\n",
    "final_output = []\n",
    "for item in results:\n",
    "    final_output.append({\n",
    "        \"start_time\": format_timestamp_properly(item[\"start_time\"]),\n",
    "        \"title\": item[\"title\"],\n",
    "        \"summary\": item[\"summary\"]\n",
    "    })\n",
    "\n",
    "with open(\"4nMwZhF_D-g_chapters.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"ğŸ‰ Final chapter file saved: 4nMwZhF_D-g_chapters.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5720ca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Contents of: 4nMwZhF_D-g_chapters.json\n",
      "â± 0:00:00 | ğŸ· class maker\n",
      "ğŸ“ Newcomers! Tea - maker, Pancher - maker, Cuddler - maker, Bar - dancer and waiter These are some professions, some jobs that some people do, but in our country these words are often used like jokes. In this short video today, I would like to talk a little bit about social evil, which is class - maker. Class - maker correctly heard its own class - maker. Class - maker on class - maker, that is, on the one hand, make a difference People of nÃºmer p removets StraÃŸe\n",
      "\n",
      "â± 0:07:03 | ğŸ· people inferiority\n",
      "ğŸ“ People ignore them because they think they are of a lower level. They think they are of a lower section in society. Do you know the most special thing about a classist society? An induction who lives in such a society. A person who lives in such a society will always look down on the people below him. Mo and the people above him will always join hands. Will always bow down to the people above him. G sir, yes madam. There will always be hierarchy. You can see the inferiority complex in front of the people above him. And it is inferiority complex in front\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Path to the folder containing final chapter JSON files\n",
    "final_folder = \"final\"  # Change if your folder is named differently\n",
    "\n",
    "# List all JSON files in the folder\n",
    "chapter_files = [f for f in os.listdir(final_folder) if f.endswith(\".json\")]\n",
    "\n",
    "# Print the content of each file\n",
    "for file in chapter_files:\n",
    "    filepath = os.path.join(final_folder, file)\n",
    "    print(f\"\\nğŸ“„ Contents of: {file}\")\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            print(f\"â± {entry['start_time']} | ğŸ· {entry['title']}\\nğŸ“ {entry['summary']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
